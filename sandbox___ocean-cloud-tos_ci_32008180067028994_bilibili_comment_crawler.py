"""
Bç«™è§†é¢‘è¯„è®ºæŠ“å–å·¥å…·
åŠŸèƒ½ï¼šæ ¹æ®Bç«™è§†é¢‘URLæŠ“å–è¯„è®ºï¼Œå¹¶ä¿å­˜ä¸ºExcelæ–‡ä»¶
æ”¯æŒï¼šåˆ†é¡µæŠ“å–ã€è¯„è®ºå»é‡ã€è¡¨æƒ…ç¬¦å·å¤„ç†ã€Excelå¯¼å‡º
"""

import requests
import pandas as pd
import time
import random
from urllib.parse import urlparse, parse_qs
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class BilibiliCommentCrawler:
    def __init__(self):
        """åˆå§‹åŒ–Bç«™è¯„è®ºçˆ¬è™«"""
        # è¯·æ±‚å¤´è®¾ç½®ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Referer': 'https://www.bilibili.com/',
            'Connection': 'keep-alive',
            'Cookie': 'buvid3=F7E50195-2708-F4F8-5D79-6290865A25EB08277infoc; b_nut=1739341126; _uuid=ED1077AFA-B241-1062D-669F-F4107EE19F13473584infoc; header_theme_version=CLOSE; enable_web_push=DISABLE; home_feed_column=5; buvid_fp=66699aac769b109c1992f3b48eb4bbd4; rpdid=|(~|)RYRm|Y0J'u~JmRl|Y)R; DedeUserID=278433342; DedeUserID__ckMd5=e42de04fc355d35f; enable_feed_channel=ENABLE; LIVE_BUVID=AUTO4317416905588586; theme-tip-show=SHOWED; buvid4=49E1D02C-98FF-12F4-F7C6-9E710B7A148D08277-025021206-9JESgZ7vuHYuslN+EUXKqw%3D%3D; theme-avatar-tip-show=SHOWED; theme-switch-show=SHOWED; CURRENT_QUALITY=80; PVID=3; browser_resolution=1603-884; bmg_af_switch=1; bmg_src_def_domain=i2.hdslb.com; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NjMxODI2NDAsImlhdCI6MTc2MjkyMzM4MCwicGx0IjotMX0.ryoNnrR5gj1MwS-YAoXiO7-sRwf5WxJyQK9xFLg6wkg; bili_ticket_expires=1763182580; SESSDATA=c79384c8%2C1778475441%2Cb10ce%2Ab1CjCJ2jMp-dceOpnrU1RS2czKj2IM7vuSwn9DvMJycTVcKIygaGM2D2jur2fv-PgdmwsSVmJ2eUhkc1VjN3E1UDlDRi1SUmJmejhuVEVkUDdmeDg5TXUtQXZHeTFYeFVxLUw2VHV5WjRWY0RUbjU3SEJ0MDZ0NVg1M2VIa0F4WWdqdlFNZjIwWk13IIEC; bili_jct=d2e6e1b4b11148d29bbc39c2cc68bd1a; sid=5hs59ulx; bp_t_offset_278433342=1134238121991340032; share_source_origin=WEIXIN; bsource=share_source_weixinchat; CURRENT_FNVAL=4048; b_lsid=71017BC4A_19A7838D34C'  # è¯·æ›¿æ¢ä¸ºè‡ªå·±çš„Cookie
        }
        
        # è¯„è®ºAPIæ¥å£
        self.comment_api = 'https://api.bilibili.com/x/v2/reply/wbi/main'
        
        # å»¶è¿Ÿè®¾ç½®ï¼Œé¿å…è¢«å°ç¦
        self.min_delay = 1.5  # æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        self.max_delay = 3.0  # æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
        # å­˜å‚¨è¯„è®ºæ•°æ®
        self.comments_data = []
        
    def get_video_cid(self, video_url):
        """
        ä»è§†é¢‘URLè·å–è§†é¢‘çš„cidï¼ˆè¯„è®ºåŒºIDï¼‰
        :param video_url: Bç«™è§†é¢‘URLï¼ˆå¦‚https://www.bilibili.com/video/BV1xx4y1V7eDï¼‰
        :return: cidæˆ–None
        """
        try:
            # è§£æURLè·å–aidæˆ–bvid
            parsed_url = urlparse(video_url)
            query_params = parse_qs(parsed_url.query)
            
            # æå–bvidï¼ˆä»URLè·¯å¾„ä¸­ï¼‰
            path_parts = parsed_url.path.split('/')
            bvid = None
            for part in path_parts:
                if part.startswith('BV'):
                    bvid = part
                    break
            
            if not bvid:
                print("âŒ æ— æ³•ä»URLä¸­æå–BVå·")
                return None
            
            # è·å–è§†é¢‘ä¿¡æ¯ï¼ŒåŒ…å«cid
            video_info_url = f'https://api.bilibili.com/x/web-interface/view?bvid={bvid}'
            response = requests.get(video_info_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('code') != 0:
                print(f"âŒ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥ï¼š{data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return None
            
            cid = data.get('data', {}).get('cid')
            if not cid:
                print("âŒ æ— æ³•è·å–è§†é¢‘çš„cid")
                return None
            
            print(f"âœ… æˆåŠŸè·å–è§†é¢‘ä¿¡æ¯ï¼šBVå·={bvid}, cid={cid}")
            return cid
            
        except Exception as e:
            print(f"âŒ è·å–cidæ—¶å‡ºé”™ï¼š{str(e)}")
            return None
    
    def crawl_comments(self, cid, max_pages=100):
        """
        æŠ“å–æŒ‡å®šcidçš„è¯„è®º
        :param cid: è§†é¢‘çš„cid
        :param max_pages: æœ€å¤§æŠ“å–é¡µæ•°ï¼ˆæ¯é¡µçº¦20æ¡è¯„è®ºï¼‰
        :return: è¯„è®ºæ•°æ®åˆ—è¡¨
        """
        if not cid:
            print("âŒ cidä¸èƒ½ä¸ºç©º")
            return []
        
        print(f"\nğŸ“¥ å¼€å§‹æŠ“å–è¯„è®ºï¼Œcid={cid}ï¼Œæœ€å¤§æŠ“å–é¡µæ•°={max_pages}")
        self.comments_data = []
        page = 1
        total_comments = 0
        
        while page <= max_pages:
            try:
                # æ„é€ è¯·æ±‚å‚æ•°
                params = {
                    'cid': cid,
                    'page': page,
                    'size': 20,  # æ¯é¡µ20æ¡è¯„è®º
                    'order': 'hot',  # æŒ‰çƒ­åº¦æ’åºï¼ˆhot-çƒ­åº¦ï¼Œtime-æ—¶é—´ï¼‰
                    'plat': 1,
                    'type': 1,
                    'oid': cid,
                    'mode': 3
                }
                
                # å‘é€è¯·æ±‚
                response = requests.get(
                    self.comment_api,
                    headers=self.headers,
                    params=params,
                    timeout=15,
                    verify=False
                )
                response.raise_for_status()
                data = response.json()
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if data.get('code') != 0:
                    print(f"âŒ ç¬¬{page}é¡µè¯„è®ºè¯·æ±‚å¤±è´¥ï¼š{data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    break
                
                # è§£æè¯„è®ºæ•°æ®
                reply_data = data.get('data', {}).get('replies', [])
                if not reply_data:
                    print(f"âœ… å·²è·å–æ‰€æœ‰è¯„è®ºï¼ˆç¬¬{page}é¡µæ— æ•°æ®ï¼‰")
                    break
                
                # æå–è¯„è®ºå†…å®¹
                for comment in reply_data:
                    comment_content = comment.get('content', {}).get('message', '').strip()
                    if comment_content:  # åªä¿ç•™éç©ºè¯„è®º
                        self.comments_data.append({
                            'content': comment_content,
                            'user_name': comment.get('member', {}).get('uname', 'æœªçŸ¥ç”¨æˆ·'),
                            'user_id': comment.get('member', {}).get('mid', 0),
                            'like_count': comment.get('like', 0),
                            'reply_count': comment.get('rcount', 0),
                            'publish_time': datetime.fromtimestamp(comment.get('ctime', 0)).strftime('%Y-%m-%d %H:%M:%S'),
                            'comment_id': comment.get('rpid', 0)
                        })
                
                # ç»Ÿè®¡è¿›åº¦
                page_comments = len(reply_data)
                total_comments += page_comments
                print(f"ğŸ“„ ç¬¬{page}é¡µæŠ“å–å®Œæˆï¼Œè·å–è¯„è®º{page_comments}æ¡ï¼Œç´¯è®¡{total_comments}æ¡")
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°ç¦
                delay = random.uniform(self.min_delay, self.max_delay)
                time.sleep(delay)
                
                page += 1
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ ç¬¬{page}é¡µè¯·æ±‚å‡ºé”™ï¼š{str(e)}")
                # å¢åŠ å»¶è¿Ÿåé‡è¯•
                time.sleep(5)
                continue
            except Exception as e:
                print(f"âŒ ç¬¬{page}é¡µè§£æå‡ºé”™ï¼š{str(e)}")
                page += 1
                time.sleep(2)
                continue
        
        print(f"\nğŸ“Š è¯„è®ºæŠ“å–å®Œæˆï¼å…±è·å–{len(self.comments_data)}æ¡æœ‰æ•ˆè¯„è®º")
        return self.comments_data
    
    def remove_duplicate_comments(self):
        """å»é™¤é‡å¤è¯„è®º"""
        if not self.comments_data:
            print("âŒ æ²¡æœ‰è¯„è®ºæ•°æ®å¯å»é‡")
            return
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œå»é‡
        df = pd.DataFrame(self.comments_data)
        original_count = len(df)
        
        # åŸºäºè¯„è®ºå†…å®¹å»é‡
        df_cleaned = df.drop_duplicates(subset=['content'], keep='first')
        cleaned_count = len(df_cleaned)
        duplicate_count = original_count - cleaned_count
        
        print(f"\nğŸ§¹ è¯„è®ºå»é‡å®Œæˆ")
        print(f"   åŸå§‹è¯„è®ºæ•°ï¼š{original_count}")
        print(f"   å»é‡åè¯„è®ºæ•°ï¼š{cleaned_count}")
        print(f"   å»é™¤é‡å¤è¯„è®ºæ•°ï¼š{duplicate_count}")
        
        # æ›´æ–°è¯„è®ºæ•°æ®
        self.comments_data = df_cleaned.to_dict('records')
        return self.comments_data
    
    def save_to_excel(self, file_name=None, only_content=True):
        """
        ä¿å­˜è¯„è®ºæ•°æ®åˆ°Excelæ–‡ä»¶
        :param file_name: æ–‡ä»¶åï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
        :param only_content: æ˜¯å¦åªä¿å­˜è¯„è®ºå†…å®¹ï¼ˆä¸ç”¨æˆ·æä¾›çš„æ•°æ®æ ¼å¼ä¸€è‡´ï¼‰
        """
        if not self.comments_data:
            print("âŒ æ²¡æœ‰è¯„è®ºæ•°æ®å¯ä¿å­˜")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(self.comments_data)
        
        # å¦‚æœåªä¿å­˜è¯„è®ºå†…å®¹
        if only_content:
            df_save = df[['content']].copy()
            print(f"ğŸ“ åªä¿å­˜è¯„è®ºå†…å®¹å­—æ®µ")
        else:
            df_save = df.copy()
            print(f"ğŸ“ ä¿å­˜å®Œæ•´è¯„è®ºæ•°æ®ï¼ˆ{len(df_save.columns)}ä¸ªå­—æ®µï¼‰")
        
        # ç”Ÿæˆæ–‡ä»¶å
        if not file_name:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            file_name = f'ã€Bç«™ã€‘bç«™è¯„è®ºæ±‡æ€»_{timestamp}.xlsx'
        
        # ä¿å­˜åˆ°Excel
        try:
            df_save.to_excel(file_name, index=False, engine='openpyxl')
            print(f"âœ… è¯„è®ºæ•°æ®å·²ä¿å­˜åˆ°ï¼š{file_name}")
            print(f"   å…±ä¿å­˜{len(df_save)}æ¡è¯„è®º")
            return file_name
        except Exception as e:
            print(f"âŒ ä¿å­˜Excelæ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
            return None
    
    def run(self, video_url, max_pages=100, only_content=True, output_file=None):
        """
        å®Œæ•´è¿è¡Œæµç¨‹
        :param video_url: Bç«™è§†é¢‘URL
        :param max_pages: æœ€å¤§æŠ“å–é¡µæ•°
        :param only_content: æ˜¯å¦åªä¿å­˜è¯„è®ºå†…å®¹
        :param output_file: è¾“å‡ºæ–‡ä»¶å
        :return: ä¿å­˜çš„æ–‡ä»¶å
        """
        print("=" * 60)
        print("ğŸ¯ Bç«™è¯„è®ºæŠ“å–å·¥å…· v1.0")
        print("=" * 60)
        
        # 1. è·å–cid
        cid = self.get_video_cid(video_url)
        if not cid:
            return None
        
        # 2. æŠ“å–è¯„è®º
        self.crawl_comments(cid, max_pages)
        
        # 3. å»é‡å¤„ç†
        self.remove_duplicate_comments()
        
        # 4. ä¿å­˜åˆ°Excel
        if self.comments_data:
            saved_file = self.save_to_excel(output_file, only_content)
            return saved_file
        else:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆè¯„è®ºå¯ä¿å­˜")
            return None


def main():
    """ç¤ºä¾‹è¿è¡Œ"""
    # 1. é…ç½®å‚æ•°
    VIDEO_URL = "https://www.bilibili.com/video/BV1xx4y1V7eD"  # æ›¿æ¢ä¸ºç›®æ ‡è§†é¢‘URL
    MAX_PAGES = 50  # æœ€å¤§æŠ“å–é¡µæ•°ï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    ONLY_CONTENT = True  # åªä¿å­˜è¯„è®ºå†…å®¹ï¼ˆä¸ç”¨æˆ·æ•°æ®æ ¼å¼ä¸€è‡´ï¼‰
    OUTPUT_FILE = "ã€Bç«™ã€‘bç«™è¯„è®ºæ±‡æ€».xlsx"  # è¾“å‡ºæ–‡ä»¶å
    
    # 2. åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = BilibiliCommentCrawler()
    
    # 3. æ³¨æ„ï¼šéœ€è¦æ›¿æ¢Cookieï¼
    print("\nâš ï¸  é‡è¦æç¤ºï¼š")
    print("   1. è¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•Bç«™åè·å–Cookie")
    print("   2. å°†Cookieæ›¿æ¢åˆ°BilibiliCommentCrawlerç±»çš„headersä¸­")
    print("   3. CookieåŒ…å«buvid3ã€bili_jctã€sidç­‰å…³é”®ä¿¡æ¯")
    print()
    
    # 4. è¿è¡Œçˆ¬è™«
    input("   æŒ‰å›è½¦é”®å¼€å§‹æŠ“å–ï¼ˆç¡®ä¿å·²é…ç½®Cookieï¼‰...")
    saved_file = crawler.run(
        video_url=VIDEO_URL,
        max_pages=MAX_PAGES,
        only_content=ONLY_CONTENT,
        output_file=OUTPUT_FILE
    )
    
    if saved_file:
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼è¯„è®ºæ–‡ä»¶ï¼š{saved_file}")
    else:
        print(f"\nâŒ ä»»åŠ¡å¤±è´¥")


if __name__ == "__main__":
    main()
