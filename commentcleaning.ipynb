{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e16674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\82588\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂéüÂßãÊï∞ÊçÆÔºö3142Êù°ËØÑËÆ∫\n",
      "Âü∫Á°ÄËøáÊª§ÂêéÔºö3123Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\n",
      "Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö3109Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.826 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\n",
      "ÂéüÂßãÊï∞ÊçÆÔºö3142Êù°\n",
      "ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö3109Êù°\n",
      "Êï∞ÊçÆ‰øùÁïôÁéáÔºö98.9%\n",
      "Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºöD:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\n",
      "\n",
      "=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫1„ÄëÔºö\n",
      "  ÊàëÂØªÊÄùÊú®‰πÉ‰ºä‰πüÊòØÊúâÁöÑÂïä\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫1„ÄëÔºö\n",
      "  ÊàëÂØªÊÄùÊú®‰πÉ‰ºä‰πüÊòØÊúâÁöÑÂïä\n",
      "„ÄêÂàÜËØçÁªìÊûú1„ÄëÔºö\n",
      "  ÂØªÊÄù Êú®‰πÉ‰ºä Âïä\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫2„ÄëÔºö\n",
      "  ÂÅöÊ†áÊú¨ÈúÄË¶ÅÂ§™Â§öÁöÑÊó∂Èó¥ÔºåÂ§™Â§öÁöÑÊµÅÁ®ãÔºÅÈúÄË¶ÅÁêÜËß£‰∏Ä‰∏ã‰ªñ‰ª¨ÔºÅ[ÁÉ≠ËØçÁ≥ªÂàó_‰øùÊä§]\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫2„ÄëÔºö\n",
      "  ÂÅöÊ†áÊú¨ÈúÄË¶ÅÂ§™Â§öÁöÑÊó∂Èó¥ÔºåÂ§™Â§öÁöÑÊµÅÁ®ãÔºÅÈúÄË¶ÅÁêÜËß£‰∏Ä‰∏ã‰ªñ‰ª¨ÔºÅ\n",
      "„ÄêÂàÜËØçÁªìÊûú2„ÄëÔºö\n",
      "  ÂÅö Ê†áÊú¨ ÈúÄË¶Å Â§™Â§ö Êó∂Èó¥ Â§™Â§ö ÊµÅÁ®ã ÈúÄË¶Å ÁêÜËß£ ‰∏Ä‰∏ã ‰ªñ‰ª¨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_bilibili_data(input_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/bilibili.xlsx\", output_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\"):\n",
    "    \"\"\"\n",
    "    ÂÆöÂà∂ÂåñÊ∏ÖÊ¥óBÁ´ôËØÑËÆ∫Êï∞ÊçÆÔºàÈÄÇÈÖç‰∏ä‰º†ÁöÑbilibili.xlsxÊñá‰ª∂Ôºâ\n",
    "    Ê†∏ÂøÉÂäüËÉΩÔºöÂéªÈáç„ÄÅÂéªÂô™„ÄÅÊñáÊú¨Ê†áÂáÜÂåñ„ÄÅÂàÜËØçÂ§ÑÁêÜ\n",
    "    \"\"\"\n",
    "    # ---------------------- 1. ËØªÂèñÂéüÂßãÊï∞ÊçÆ ----------------------\n",
    "    print(\"Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\")\n",
    "    df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    original_count = len(df)\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°ËØÑËÆ∫\")\n",
    "\n",
    "    # ---------------------- 2. Âü∫Á°ÄÊï∞ÊçÆËøáÊª§ ----------------------\n",
    "    # 2.1 Âà†Èô§ÂÆåÂÖ®ÈáçÂ§çÁöÑËØÑËÆ∫\n",
    "    df = df.drop_duplicates(subset=[\"content\"])\n",
    "    # 2.2 Âà†Èô§ËØÑËÆ∫ÂÜÖÂÆπ‰∏∫Á©∫/NaNÁöÑÊù°ÁõÆ\n",
    "    df = df.dropna(subset=[\"content\"])\n",
    "    # 2.3 ËøáÊª§Â≠óÊï∞ËøáÁü≠ÁöÑÊó†ÊÑè‰πâËØÑËÆ∫ÔºàÂ∞ë‰∫é2Â≠óÔºâ\n",
    "    df = df[df[\"content\"].str.len() >= 2]\n",
    "    # 2.4 ÈáçÁΩÆÁ¥¢ÂºïÔºàÈÅøÂÖçÊ∏ÖÊ¥óÂêéÁ¥¢Âºï‰∏çËøûÁª≠Ôºâ\n",
    "    df = df.reset_index(drop=True)\n",
    "    basic_clean_count = len(df)\n",
    "    print(f\"Âü∫Á°ÄËøáÊª§ÂêéÔºö{basic_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\")\n",
    "\n",
    "    # ---------------------- 3. Ê∑±Â∫¶ÊñáÊú¨Ê∏ÖÊ¥óÔºàÂéªÂô™+Ê†áÂáÜÂåñÔºâ ----------------------\n",
    "    def clean_single_comment(text):\n",
    "        \"\"\"ÂçïÊù°ËØÑËÆ∫Ê∏ÖÊ¥óÔºöÂéªÈô§BÁ´ôÁâπÊúâÂô™Â£∞ÔºåÁªü‰∏ÄÊ†ºÂºè\"\"\"\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # 3.1 ÂéªÈô§BÁ´ôÁâπÊÆäÂÖÉÁ¥†\n",
    "        text = re.sub(r\"\\[tv_\\w+\\]|\\[ÁÉ≠ËØçÁ≥ªÂàó_\\w+\\]|\\[\\w+\\]\", \"\", text)  # ÂéªÈô§Ë°®ÊÉÖÔºàÂ¶Ç[tv_ÁÇπËµû]Ôºâ\n",
    "        text = re.sub(r\"https?://\\S+|b23.tv/\\S+\", \"\", text)              # ÂéªÈô§URLÈìæÊé•ÔºàÂê´BÁ´ôÁü≠ÈìæÔºâ\n",
    "        text = re.sub(r\"@\\w+|#\\w+#\", \"\", text)                            # ÂéªÈô§@Áî®Êà∑Âíå#ËØùÈ¢òÊ†áÁ≠æ\n",
    "        \n",
    "        # 3.2 ÊñáÊú¨Ê†ºÂºèÊ†áÂáÜÂåñ\n",
    "        text = re.sub(r\"\\n+\", \"„ÄÇ\", text)  # Êç¢Ë°åÁ¨¶ËΩ¨‰∏∫Âè•Âè∑ÔºàÈÅøÂÖçÊñ≠Âè•Ê∑∑‰π±Ôºâ\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # Â§ö‰∏™Á©∫Ê†ºÂêàÂπ∂‰∏∫‰∏Ä‰∏™\n",
    "        text = text.replace(\",\", \"Ôºå\").replace(\".\", \"„ÄÇ\").replace(\"?\", \"Ôºü\").replace(\"!\", \"ÔºÅ\")  # Áªü‰∏Ä‰∏≠ÊñáÊ†áÁÇπ\n",
    "        \n",
    "        # 3.3 ‰øùÁïôÊúâÊïàÂ≠óÁ¨¶Ôºà‰ªÖ‰øùÁïô‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊï∞Â≠ó„ÄÅÂ∏∏Áî®‰∏≠ÊñáÊ†áÁÇπÔºâ\n",
    "        text = re.sub(r\"[^\\u4e00-\\u9fa5a-zA-Z0-9\\sÔºå„ÄÇÔºÅÔºüÔºõÔºö‚Äú‚Äù‚Äò‚ÄôÔºàÔºâ„Äê„Äë„ÄÅ]\", \"\", text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Â∫îÁî®Ê∏ÖÊ¥óÂáΩÊï∞ÔºåÊñ∞Â¢û‚ÄúÊ∏ÖÊ¥óÂêéËØÑËÆ∫‚ÄùÂ≠óÊÆµ\n",
    "    df[\"cleaned_content\"] = df[\"content\"].apply(clean_single_comment)\n",
    "    \n",
    "    # ËøáÊª§Ê∏ÖÊ¥óÂêé‰∏∫Á©∫/‰ªçËøáÁü≠ÁöÑËØÑËÆ∫ÔºàÂ¶ÇÂéüËØÑËÆ∫ÂÖ®ÊòØË°®ÊÉÖ/ÈìæÊé•Ôºâ\n",
    "    df = df[df[\"cleaned_content\"].str.len() >= 2]\n",
    "    df = df.reset_index(drop=True)\n",
    "    deep_clean_count = len(df)\n",
    "    print(f\"Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö{deep_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\")\n",
    "\n",
    "    # ---------------------- 4. ÂàÜËØçÂ§ÑÁêÜÔºà‰æø‰∫éÂêéÁª≠ÂàÜÊûêÔºâ ----------------------\n",
    "    # Âä†ËΩΩÂÅúÁî®ËØçË°®ÔºàËøáÊª§Êó†ÊÑè‰πâËØçÊ±áÔºå‰øùÁïôÊ†∏ÂøÉËØ≠‰πâÔºâ\n",
    "    stopwords = {\n",
    "        \"ÁöÑ\", \"‰∫Ü\", \"ÊòØ\", \"Êàë\", \"‰Ω†\", \"‰ªñ\", \"Â•π\", \"ÂÆÉ\", \"‰ª¨\", \"Âú®\", \"Âíå\", \"Â∞±\", \"ÈÉΩ\", \"ËÄå\", \"Âèä\", \"‰∏é\",\n",
    "        \"‰πü\", \"Ëøò\", \"‰∏ç\", \"Ê≤°\", \"Êúâ\", \"ÁùÄ\", \"Ëøá\", \"Ë¶Å\", \"‰ºö\", \"ËÉΩ\", \"ÂèØ\", \"Ëøô\", \"ÈÇ£\", \"Ê≠§\", \"ÂΩº\",\n",
    "        \"Âæà\", \"ÈùûÂ∏∏\", \"ÊØîËæÉ\", \"Â§™\", \"ÊúÄ\", \"Êõ¥\", \"Âèà\", \"ÂÜç\", \"Êâç\", \"Âè™\", \"‰ΩÜ\", \"Âç¥\", \"ËôΩ\", \"ÁÑ∂\",\n",
    "        \"Ôºå\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"Ôºö\", \"‚Äú\", \"‚Äù\", \"‚Äò\", \"‚Äô\", \"Ôºà\", \"Ôºâ\", \"„Äê\", \"„Äë\", \"„ÄÅ\", \" \",\n",
    "        \"ÂõûÂ§ç\", \"ÂºïÁî®\", \"‰∏æÊä•\", \"Âà†Èô§\", \"ÁºñËæë\", \"‰∏Ä‰∏™\", \"‰∏Ä‰∫õ\", \"‰∏ÄÁÇπ\", \"‰∏ÄÊ†∑\"\n",
    "    }\n",
    "\n",
    "    def segment_text(text):\n",
    "        \"\"\"‰∏≠ÊñáÂàÜËØçÂπ∂ËøáÊª§ÂÅúÁî®ËØç\"\"\"\n",
    "        words = jieba.lcut(text, cut_all=False)  # Á≤æÁ°ÆÂàÜËØç\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word not in stopwords \n",
    "            and len(word) >= 1 \n",
    "            and not (word.isdigit() and len(word) < 5)  # ‰øùÁïôÈïøÂ∫¶‚â•5ÁöÑÊï∞Â≠óÔºàÂ¶ÇÂπ¥‰ªΩÔºâ\n",
    "        ]\n",
    "        return filtered_words\n",
    "\n",
    "    # Êñ∞Â¢ûÂàÜËØçÁõ∏ÂÖ≥Â≠óÊÆµ\n",
    "    df[\"segmented_words\"] = df[\"cleaned_content\"].apply(segment_text)  # ÂàóË°®Ê†ºÂºèÔºà‰æø‰∫éÁ®ãÂ∫èÂ§ÑÁêÜÔºâ\n",
    "    df[\"segmented_text\"] = df[\"segmented_words\"].apply(lambda x: \" \".join(x))  # Â≠óÁ¨¶‰∏≤Ê†ºÂºèÔºà‰æø‰∫éÊü•ÁúãÔºâ\n",
    "\n",
    "    # ---------------------- 5. ‰øùÂ≠òÊ∏ÖÊ¥óÁªìÊûú ----------------------\n",
    "    # ‰øùÁïôÊ†∏ÂøÉÂ≠óÊÆµÔºöÂéüÂßãËØÑËÆ∫„ÄÅÊ∏ÖÊ¥óÂêéËØÑËÆ∫„ÄÅÂàÜËØçÁªìÊûúÔºàÂàóË°®Ôºâ„ÄÅÂàÜËØçÁªìÊûúÔºàÂ≠óÁ¨¶‰∏≤Ôºâ\n",
    "    core_columns = [\"content\", \"cleaned_content\", \"segmented_words\", \"segmented_text\"]\n",
    "    df_cleaned = df[core_columns]\n",
    "    df_cleaned.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    # ---------------------- 6. ËæìÂá∫Ê∏ÖÊ¥óÊä•Âëä ----------------------\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\")\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°\")\n",
    "    print(f\"ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö{deep_clean_count}Êù°\")\n",
    "    print(f\"Êï∞ÊçÆ‰øùÁïôÁéáÔºö{deep_clean_count/original_count*100:.1f}%\")\n",
    "    print(f\"Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºö{output_path}\")\n",
    "    \n",
    "    # Â±ïÁ§∫Ââç2Êù°Ê∏ÖÊ¥óÊïàÊûúÂØπÊØî\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\")\n",
    "    for i in range(min(2, len(df_cleaned))):\n",
    "        print(f\"\\n„ÄêÂéüÂßãËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        original = df_cleaned.iloc[i][\"content\"]\n",
    "        print(f\"  {original[:100]}{'...' if len(original) > 100 else ''}\")\n",
    "        print(f\"„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        cleaned = df_cleaned.iloc[i][\"cleaned_content\"]\n",
    "        print(f\"  {cleaned[:100]}{'...' if len(cleaned) > 100 else ''}\")\n",
    "        print(f\"„ÄêÂàÜËØçÁªìÊûú{i+1}„ÄëÔºö\")\n",
    "        segmented = df_cleaned.iloc[i][\"segmented_text\"]\n",
    "        print(f\"  {segmented[:50]}{'...' if len(segmented) > 50 else ''}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# ---------------------- Áõ¥Êé•ËøêË°åÊ∏ÖÊ¥ó ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = clean_bilibili_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3a7257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\82588\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂéüÂßãÊï∞ÊçÆÔºö691Êù°ËØÑËÆ∫\n",
      "Âü∫Á°ÄËøáÊª§ÂêéÔºö676Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\n",
      "Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö663Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.671 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\n",
      "ÂéüÂßãÊï∞ÊçÆÔºö691Êù°\n",
      "ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö663Êù°\n",
      "Êï∞ÊçÆ‰øùÁïôÁéáÔºö95.9%\n",
      "Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºöD:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\n",
      "\n",
      "=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫1„ÄëÔºö\n",
      "  „ÄÇÂöéÂïïÂ§ßÂì≠\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫1„ÄëÔºö\n",
      "  „ÄÇÂöéÂïïÂ§ßÂì≠\n",
      "„ÄêÂàÜËØçÁªìÊûú1„ÄëÔºö\n",
      "  ÂöéÂïïÂ§ßÂì≠\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫2„ÄëÔºö\n",
      "  ÁúãÂì≠‰∫Üüò≠ÂÆ∂Èáå‰πüÊúâ‰∏ÄÂè™ÊØõÂ≠©Â≠êÔºåÊú™Êù•ÂæóÊúâÂ§öËàç‰∏çÂæó\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫2„ÄëÔºö\n",
      "  ÁúãÂì≠‰∫ÜÂÆ∂Èáå‰πüÊúâ‰∏ÄÂè™ÊØõÂ≠©Â≠êÔºåÊú™Êù•ÂæóÊúâÂ§öËàç‰∏çÂæó\n",
      "„ÄêÂàÜËØçÁªìÊûú2„ÄëÔºö\n",
      "  Áúã Âì≠ ÂÆ∂Èáå ‰∏ÄÂè™ ÊØõÂ≠©Â≠ê Êú™Êù• Âæó Â§ö Ëàç‰∏çÂæó\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_bilibili_data(input_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/comment4.xlsx\", output_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\"):\n",
    "    \"\"\"\n",
    "    ÂÆöÂà∂ÂåñÊ∏ÖÊ¥óBÁ´ôËØÑËÆ∫Êï∞ÊçÆÔºàÈÄÇÈÖç‰∏ä‰º†ÁöÑbilibili.xlsxÊñá‰ª∂Ôºâ\n",
    "    Ê†∏ÂøÉÂäüËÉΩÔºöÂéªÈáç„ÄÅÂéªÂô™„ÄÅÊñáÊú¨Ê†áÂáÜÂåñ„ÄÅÂàÜËØçÂ§ÑÁêÜ\n",
    "    \"\"\"\n",
    "    # ---------------------- 1. ËØªÂèñÂéüÂßãÊï∞ÊçÆ ----------------------\n",
    "    print(\"Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\")\n",
    "    df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    original_count = len(df)\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°ËØÑËÆ∫\")\n",
    "\n",
    "    # ---------------------- 2. Âü∫Á°ÄÊï∞ÊçÆËøáÊª§ ----------------------\n",
    "    # 2.1 Âà†Èô§ÂÆåÂÖ®ÈáçÂ§çÁöÑËØÑËÆ∫\n",
    "    df = df.drop_duplicates(subset=[\"content\"])\n",
    "    # 2.2 Âà†Èô§ËØÑËÆ∫ÂÜÖÂÆπ‰∏∫Á©∫/NaNÁöÑÊù°ÁõÆ\n",
    "    df = df.dropna(subset=[\"content\"])\n",
    "    # 2.3 ËøáÊª§Â≠óÊï∞ËøáÁü≠ÁöÑÊó†ÊÑè‰πâËØÑËÆ∫ÔºàÂ∞ë‰∫é2Â≠óÔºâ\n",
    "    df = df[df[\"content\"].str.len() >= 2]\n",
    "    # 2.4 ÈáçÁΩÆÁ¥¢ÂºïÔºàÈÅøÂÖçÊ∏ÖÊ¥óÂêéÁ¥¢Âºï‰∏çËøûÁª≠Ôºâ\n",
    "    df = df.reset_index(drop=True)\n",
    "    basic_clean_count = len(df)\n",
    "    print(f\"Âü∫Á°ÄËøáÊª§ÂêéÔºö{basic_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\")\n",
    "\n",
    "    # ---------------------- 3. Ê∑±Â∫¶ÊñáÊú¨Ê∏ÖÊ¥óÔºàÂéªÂô™+Ê†áÂáÜÂåñÔºâ ----------------------\n",
    "    def clean_single_comment(text):\n",
    "        \"\"\"ÂçïÊù°ËØÑËÆ∫Ê∏ÖÊ¥óÔºöÂéªÈô§BÁ´ôÁâπÊúâÂô™Â£∞ÔºåÁªü‰∏ÄÊ†ºÂºè\"\"\"\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # 3.1 ÂéªÈô§BÁ´ôÁâπÊÆäÂÖÉÁ¥†\n",
    "        text = re.sub(r\"\\[tv_\\w+\\]|\\[ÁÉ≠ËØçÁ≥ªÂàó_\\w+\\]|\\[\\w+\\]\", \"\", text)  # ÂéªÈô§Ë°®ÊÉÖÔºàÂ¶Ç[tv_ÁÇπËµû]Ôºâ\n",
    "        text = re.sub(r\"https?://\\S+|b23.tv/\\S+\", \"\", text)              # ÂéªÈô§URLÈìæÊé•ÔºàÂê´BÁ´ôÁü≠ÈìæÔºâ\n",
    "        text = re.sub(r\"@\\w+|#\\w+#\", \"\", text)                            # ÂéªÈô§@Áî®Êà∑Âíå#ËØùÈ¢òÊ†áÁ≠æ\n",
    "        \n",
    "        # 3.2 ÊñáÊú¨Ê†ºÂºèÊ†áÂáÜÂåñ\n",
    "        text = re.sub(r\"\\n+\", \"„ÄÇ\", text)  # Êç¢Ë°åÁ¨¶ËΩ¨‰∏∫Âè•Âè∑ÔºàÈÅøÂÖçÊñ≠Âè•Ê∑∑‰π±Ôºâ\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # Â§ö‰∏™Á©∫Ê†ºÂêàÂπ∂‰∏∫‰∏Ä‰∏™\n",
    "        text = text.replace(\",\", \"Ôºå\").replace(\".\", \"„ÄÇ\").replace(\"?\", \"Ôºü\").replace(\"!\", \"ÔºÅ\")  # Áªü‰∏Ä‰∏≠ÊñáÊ†áÁÇπ\n",
    "        \n",
    "        # 3.3 ‰øùÁïôÊúâÊïàÂ≠óÁ¨¶Ôºà‰ªÖ‰øùÁïô‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊï∞Â≠ó„ÄÅÂ∏∏Áî®‰∏≠ÊñáÊ†áÁÇπÔºâ\n",
    "        text = re.sub(r\"[^\\u4e00-\\u9fa5a-zA-Z0-9\\sÔºå„ÄÇÔºÅÔºüÔºõÔºö‚Äú‚Äù‚Äò‚ÄôÔºàÔºâ„Äê„Äë„ÄÅ]\", \"\", text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Â∫îÁî®Ê∏ÖÊ¥óÂáΩÊï∞ÔºåÊñ∞Â¢û‚ÄúÊ∏ÖÊ¥óÂêéËØÑËÆ∫‚ÄùÂ≠óÊÆµ\n",
    "    df[\"cleaned_content\"] = df[\"content\"].apply(clean_single_comment)\n",
    "    \n",
    "    # ËøáÊª§Ê∏ÖÊ¥óÂêé‰∏∫Á©∫/‰ªçËøáÁü≠ÁöÑËØÑËÆ∫ÔºàÂ¶ÇÂéüËØÑËÆ∫ÂÖ®ÊòØË°®ÊÉÖ/ÈìæÊé•Ôºâ\n",
    "    df = df[df[\"cleaned_content\"].str.len() >= 2]\n",
    "    df = df.reset_index(drop=True)\n",
    "    deep_clean_count = len(df)\n",
    "    print(f\"Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö{deep_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\")\n",
    "\n",
    "    # ---------------------- 4. ÂàÜËØçÂ§ÑÁêÜÔºà‰æø‰∫éÂêéÁª≠ÂàÜÊûêÔºâ ----------------------\n",
    "    # Âä†ËΩΩÂÅúÁî®ËØçË°®ÔºàËøáÊª§Êó†ÊÑè‰πâËØçÊ±áÔºå‰øùÁïôÊ†∏ÂøÉËØ≠‰πâÔºâ\n",
    "    stopwords = {\n",
    "        \"ÁöÑ\", \"‰∫Ü\", \"ÊòØ\", \"Êàë\", \"‰Ω†\", \"‰ªñ\", \"Â•π\", \"ÂÆÉ\", \"‰ª¨\", \"Âú®\", \"Âíå\", \"Â∞±\", \"ÈÉΩ\", \"ËÄå\", \"Âèä\", \"‰∏é\",\n",
    "        \"‰πü\", \"Ëøò\", \"‰∏ç\", \"Ê≤°\", \"Êúâ\", \"ÁùÄ\", \"Ëøá\", \"Ë¶Å\", \"‰ºö\", \"ËÉΩ\", \"ÂèØ\", \"Ëøô\", \"ÈÇ£\", \"Ê≠§\", \"ÂΩº\",\n",
    "        \"Âæà\", \"ÈùûÂ∏∏\", \"ÊØîËæÉ\", \"Â§™\", \"ÊúÄ\", \"Êõ¥\", \"Âèà\", \"ÂÜç\", \"Êâç\", \"Âè™\", \"‰ΩÜ\", \"Âç¥\", \"ËôΩ\", \"ÁÑ∂\",\n",
    "        \"Ôºå\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"Ôºö\", \"‚Äú\", \"‚Äù\", \"‚Äò\", \"‚Äô\", \"Ôºà\", \"Ôºâ\", \"„Äê\", \"„Äë\", \"„ÄÅ\", \" \",\n",
    "        \"ÂõûÂ§ç\", \"ÂºïÁî®\", \"‰∏æÊä•\", \"Âà†Èô§\", \"ÁºñËæë\", \"‰∏Ä‰∏™\", \"‰∏Ä‰∫õ\", \"‰∏ÄÁÇπ\", \"‰∏ÄÊ†∑\"\n",
    "    }\n",
    "\n",
    "    def segment_text(text):\n",
    "        \"\"\"‰∏≠ÊñáÂàÜËØçÂπ∂ËøáÊª§ÂÅúÁî®ËØç\"\"\"\n",
    "        words = jieba.lcut(text, cut_all=False)  # Á≤æÁ°ÆÂàÜËØç\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word not in stopwords \n",
    "            and len(word) >= 1 \n",
    "            and not (word.isdigit() and len(word) < 5)  # ‰øùÁïôÈïøÂ∫¶‚â•5ÁöÑÊï∞Â≠óÔºàÂ¶ÇÂπ¥‰ªΩÔºâ\n",
    "        ]\n",
    "        return filtered_words\n",
    "\n",
    "    # Êñ∞Â¢ûÂàÜËØçÁõ∏ÂÖ≥Â≠óÊÆµ\n",
    "    df[\"segmented_words\"] = df[\"cleaned_content\"].apply(segment_text)  # ÂàóË°®Ê†ºÂºèÔºà‰æø‰∫éÁ®ãÂ∫èÂ§ÑÁêÜÔºâ\n",
    "    df[\"segmented_text\"] = df[\"segmented_words\"].apply(lambda x: \" \".join(x))  # Â≠óÁ¨¶‰∏≤Ê†ºÂºèÔºà‰æø‰∫éÊü•ÁúãÔºâ\n",
    "\n",
    "    # ---------------------- 5. ‰øùÂ≠òÊ∏ÖÊ¥óÁªìÊûú ----------------------\n",
    "    # ‰øùÁïôÊ†∏ÂøÉÂ≠óÊÆµÔºöÂéüÂßãËØÑËÆ∫„ÄÅÊ∏ÖÊ¥óÂêéËØÑËÆ∫„ÄÅÂàÜËØçÁªìÊûúÔºàÂàóË°®Ôºâ„ÄÅÂàÜËØçÁªìÊûúÔºàÂ≠óÁ¨¶‰∏≤Ôºâ\n",
    "    core_columns = [\"content\", \"cleaned_content\", \"segmented_words\", \"segmented_text\"]\n",
    "    df_cleaned = df[core_columns]\n",
    "    df_cleaned.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    # ---------------------- 6. ËæìÂá∫Ê∏ÖÊ¥óÊä•Âëä ----------------------\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\")\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°\")\n",
    "    print(f\"ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö{deep_clean_count}Êù°\")\n",
    "    print(f\"Êï∞ÊçÆ‰øùÁïôÁéáÔºö{deep_clean_count/original_count*100:.1f}%\")\n",
    "    print(f\"Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºö{output_path}\")\n",
    "    \n",
    "    # Â±ïÁ§∫Ââç2Êù°Ê∏ÖÊ¥óÊïàÊûúÂØπÊØî\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\")\n",
    "    for i in range(min(2, len(df_cleaned))):\n",
    "        print(f\"\\n„ÄêÂéüÂßãËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        original = df_cleaned.iloc[i][\"content\"]\n",
    "        print(f\"  {original[:100]}{'...' if len(original) > 100 else ''}\")\n",
    "        print(f\"„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        cleaned = df_cleaned.iloc[i][\"cleaned_content\"]\n",
    "        print(f\"  {cleaned[:100]}{'...' if len(cleaned) > 100 else ''}\")\n",
    "        print(f\"„ÄêÂàÜËØçÁªìÊûú{i+1}„ÄëÔºö\")\n",
    "        segmented = df_cleaned.iloc[i][\"segmented_text\"]\n",
    "        print(f\"  {segmented[:50]}{'...' if len(segmented) > 50 else ''}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# ---------------------- Áõ¥Êé•ËøêË°åÊ∏ÖÊ¥ó ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = clean_bilibili_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376e2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\82588\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂéüÂßãÊï∞ÊçÆÔºö179Êù°ËØÑËÆ∫\n",
      "Âü∫Á°ÄËøáÊª§ÂêéÔºö179Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\n",
      "Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö176Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.788 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\n",
      "ÂéüÂßãÊï∞ÊçÆÔºö179Êù°\n",
      "ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö176Êù°\n",
      "Êï∞ÊçÆ‰øùÁïôÁéáÔºö98.3%\n",
      "Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºöD:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\n",
      "\n",
      "=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫1„ÄëÔºö\n",
      "  ÊàëÁöÑÁ¨¨‰∏ÄÂèçÂ∫îÂ∞±ÊòØÔºÅ ËøôËøòÊòØ‰∏ÄÁâáËìùÊµ∑ÂïäÔºÅ[ÂæÆÁ¨ë]\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫1„ÄëÔºö\n",
      "  ÊàëÁöÑÁ¨¨‰∏ÄÂèçÂ∫îÂ∞±ÊòØÔºÅ ËøôËøòÊòØ‰∏ÄÁâáËìùÊµ∑ÂïäÔºÅ\n",
      "„ÄêÂàÜËØçÁªìÊûú1„ÄëÔºö\n",
      "  Á¨¨‰∏Ä ÂèçÂ∫î Â∞±ÊòØ ËøòÊòØ ‰∏ÄÁâá ËìùÊµ∑ Âïä\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫2„ÄëÔºö\n",
      "  ÊØè‰∏™ÁîüÂëΩÈÉΩÂèØË¥µ\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫2„ÄëÔºö\n",
      "  ÊØè‰∏™ÁîüÂëΩÈÉΩÂèØË¥µ\n",
      "„ÄêÂàÜËØçÁªìÊûú2„ÄëÔºö\n",
      "  ÊØè‰∏™ ÁîüÂëΩ ÂèØË¥µ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_bilibili_data(input_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/comment5.xlsx\", output_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\"):\n",
    "    \"\"\"\n",
    "    ÂÆöÂà∂ÂåñÊ∏ÖÊ¥óBÁ´ôËØÑËÆ∫Êï∞ÊçÆÔºàÈÄÇÈÖç‰∏ä‰º†ÁöÑbilibili.xlsxÊñá‰ª∂Ôºâ\n",
    "    Ê†∏ÂøÉÂäüËÉΩÔºöÂéªÈáç„ÄÅÂéªÂô™„ÄÅÊñáÊú¨Ê†áÂáÜÂåñ„ÄÅÂàÜËØçÂ§ÑÁêÜ\n",
    "    \"\"\"\n",
    "    # ---------------------- 1. ËØªÂèñÂéüÂßãÊï∞ÊçÆ ----------------------\n",
    "    print(\"Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\")\n",
    "    df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    original_count = len(df)\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°ËØÑËÆ∫\")\n",
    "\n",
    "    # ---------------------- 2. Âü∫Á°ÄÊï∞ÊçÆËøáÊª§ ----------------------\n",
    "    # 2.1 Âà†Èô§ÂÆåÂÖ®ÈáçÂ§çÁöÑËØÑËÆ∫\n",
    "    df = df.drop_duplicates(subset=[\"content\"])\n",
    "    # 2.2 Âà†Èô§ËØÑËÆ∫ÂÜÖÂÆπ‰∏∫Á©∫/NaNÁöÑÊù°ÁõÆ\n",
    "    df = df.dropna(subset=[\"content\"])\n",
    "    # 2.3 ËøáÊª§Â≠óÊï∞ËøáÁü≠ÁöÑÊó†ÊÑè‰πâËØÑËÆ∫ÔºàÂ∞ë‰∫é2Â≠óÔºâ\n",
    "    df = df[df[\"content\"].str.len() >= 2]\n",
    "    # 2.4 ÈáçÁΩÆÁ¥¢ÂºïÔºàÈÅøÂÖçÊ∏ÖÊ¥óÂêéÁ¥¢Âºï‰∏çËøûÁª≠Ôºâ\n",
    "    df = df.reset_index(drop=True)\n",
    "    basic_clean_count = len(df)\n",
    "    print(f\"Âü∫Á°ÄËøáÊª§ÂêéÔºö{basic_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\")\n",
    "\n",
    "    # ---------------------- 3. Ê∑±Â∫¶ÊñáÊú¨Ê∏ÖÊ¥óÔºàÂéªÂô™+Ê†áÂáÜÂåñÔºâ ----------------------\n",
    "    def clean_single_comment(text):\n",
    "        \"\"\"ÂçïÊù°ËØÑËÆ∫Ê∏ÖÊ¥óÔºöÂéªÈô§BÁ´ôÁâπÊúâÂô™Â£∞ÔºåÁªü‰∏ÄÊ†ºÂºè\"\"\"\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # 3.1 ÂéªÈô§BÁ´ôÁâπÊÆäÂÖÉÁ¥†\n",
    "        text = re.sub(r\"\\[tv_\\w+\\]|\\[ÁÉ≠ËØçÁ≥ªÂàó_\\w+\\]|\\[\\w+\\]\", \"\", text)  # ÂéªÈô§Ë°®ÊÉÖÔºàÂ¶Ç[tv_ÁÇπËµû]Ôºâ\n",
    "        text = re.sub(r\"https?://\\S+|b23.tv/\\S+\", \"\", text)              # ÂéªÈô§URLÈìæÊé•ÔºàÂê´BÁ´ôÁü≠ÈìæÔºâ\n",
    "        text = re.sub(r\"@\\w+|#\\w+#\", \"\", text)                            # ÂéªÈô§@Áî®Êà∑Âíå#ËØùÈ¢òÊ†áÁ≠æ\n",
    "        \n",
    "        # 3.2 ÊñáÊú¨Ê†ºÂºèÊ†áÂáÜÂåñ\n",
    "        text = re.sub(r\"\\n+\", \"„ÄÇ\", text)  # Êç¢Ë°åÁ¨¶ËΩ¨‰∏∫Âè•Âè∑ÔºàÈÅøÂÖçÊñ≠Âè•Ê∑∑‰π±Ôºâ\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # Â§ö‰∏™Á©∫Ê†ºÂêàÂπ∂‰∏∫‰∏Ä‰∏™\n",
    "        text = text.replace(\",\", \"Ôºå\").replace(\".\", \"„ÄÇ\").replace(\"?\", \"Ôºü\").replace(\"!\", \"ÔºÅ\")  # Áªü‰∏Ä‰∏≠ÊñáÊ†áÁÇπ\n",
    "        \n",
    "        # 3.3 ‰øùÁïôÊúâÊïàÂ≠óÁ¨¶Ôºà‰ªÖ‰øùÁïô‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊï∞Â≠ó„ÄÅÂ∏∏Áî®‰∏≠ÊñáÊ†áÁÇπÔºâ\n",
    "        text = re.sub(r\"[^\\u4e00-\\u9fa5a-zA-Z0-9\\sÔºå„ÄÇÔºÅÔºüÔºõÔºö‚Äú‚Äù‚Äò‚ÄôÔºàÔºâ„Äê„Äë„ÄÅ]\", \"\", text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Â∫îÁî®Ê∏ÖÊ¥óÂáΩÊï∞ÔºåÊñ∞Â¢û‚ÄúÊ∏ÖÊ¥óÂêéËØÑËÆ∫‚ÄùÂ≠óÊÆµ\n",
    "    df[\"cleaned_content\"] = df[\"content\"].apply(clean_single_comment)\n",
    "    \n",
    "    # ËøáÊª§Ê∏ÖÊ¥óÂêé‰∏∫Á©∫/‰ªçËøáÁü≠ÁöÑËØÑËÆ∫ÔºàÂ¶ÇÂéüËØÑËÆ∫ÂÖ®ÊòØË°®ÊÉÖ/ÈìæÊé•Ôºâ\n",
    "    df = df[df[\"cleaned_content\"].str.len() >= 2]\n",
    "    df = df.reset_index(drop=True)\n",
    "    deep_clean_count = len(df)\n",
    "    print(f\"Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö{deep_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\")\n",
    "\n",
    "    # ---------------------- 4. ÂàÜËØçÂ§ÑÁêÜÔºà‰æø‰∫éÂêéÁª≠ÂàÜÊûêÔºâ ----------------------\n",
    "    # Âä†ËΩΩÂÅúÁî®ËØçË°®ÔºàËøáÊª§Êó†ÊÑè‰πâËØçÊ±áÔºå‰øùÁïôÊ†∏ÂøÉËØ≠‰πâÔºâ\n",
    "    stopwords = {\n",
    "        \"ÁöÑ\", \"‰∫Ü\", \"ÊòØ\", \"Êàë\", \"‰Ω†\", \"‰ªñ\", \"Â•π\", \"ÂÆÉ\", \"‰ª¨\", \"Âú®\", \"Âíå\", \"Â∞±\", \"ÈÉΩ\", \"ËÄå\", \"Âèä\", \"‰∏é\",\n",
    "        \"‰πü\", \"Ëøò\", \"‰∏ç\", \"Ê≤°\", \"Êúâ\", \"ÁùÄ\", \"Ëøá\", \"Ë¶Å\", \"‰ºö\", \"ËÉΩ\", \"ÂèØ\", \"Ëøô\", \"ÈÇ£\", \"Ê≠§\", \"ÂΩº\",\n",
    "        \"Âæà\", \"ÈùûÂ∏∏\", \"ÊØîËæÉ\", \"Â§™\", \"ÊúÄ\", \"Êõ¥\", \"Âèà\", \"ÂÜç\", \"Êâç\", \"Âè™\", \"‰ΩÜ\", \"Âç¥\", \"ËôΩ\", \"ÁÑ∂\",\n",
    "        \"Ôºå\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"Ôºö\", \"‚Äú\", \"‚Äù\", \"‚Äò\", \"‚Äô\", \"Ôºà\", \"Ôºâ\", \"„Äê\", \"„Äë\", \"„ÄÅ\", \" \",\n",
    "        \"ÂõûÂ§ç\", \"ÂºïÁî®\", \"‰∏æÊä•\", \"Âà†Èô§\", \"ÁºñËæë\", \"‰∏Ä‰∏™\", \"‰∏Ä‰∫õ\", \"‰∏ÄÁÇπ\", \"‰∏ÄÊ†∑\"\n",
    "    }\n",
    "\n",
    "    def segment_text(text):\n",
    "        \"\"\"‰∏≠ÊñáÂàÜËØçÂπ∂ËøáÊª§ÂÅúÁî®ËØç\"\"\"\n",
    "        words = jieba.lcut(text, cut_all=False)  # Á≤æÁ°ÆÂàÜËØç\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word not in stopwords \n",
    "            and len(word) >= 1 \n",
    "            and not (word.isdigit() and len(word) < 5)  # ‰øùÁïôÈïøÂ∫¶‚â•5ÁöÑÊï∞Â≠óÔºàÂ¶ÇÂπ¥‰ªΩÔºâ\n",
    "        ]\n",
    "        return filtered_words\n",
    "\n",
    "    # Êñ∞Â¢ûÂàÜËØçÁõ∏ÂÖ≥Â≠óÊÆµ\n",
    "    df[\"segmented_words\"] = df[\"cleaned_content\"].apply(segment_text)  # ÂàóË°®Ê†ºÂºèÔºà‰æø‰∫éÁ®ãÂ∫èÂ§ÑÁêÜÔºâ\n",
    "    df[\"segmented_text\"] = df[\"segmented_words\"].apply(lambda x: \" \".join(x))  # Â≠óÁ¨¶‰∏≤Ê†ºÂºèÔºà‰æø‰∫éÊü•ÁúãÔºâ\n",
    "\n",
    "    # ---------------------- 5. ‰øùÂ≠òÊ∏ÖÊ¥óÁªìÊûú ----------------------\n",
    "    # ‰øùÁïôÊ†∏ÂøÉÂ≠óÊÆµÔºöÂéüÂßãËØÑËÆ∫„ÄÅÊ∏ÖÊ¥óÂêéËØÑËÆ∫„ÄÅÂàÜËØçÁªìÊûúÔºàÂàóË°®Ôºâ„ÄÅÂàÜËØçÁªìÊûúÔºàÂ≠óÁ¨¶‰∏≤Ôºâ\n",
    "    core_columns = [\"content\", \"cleaned_content\", \"segmented_words\", \"segmented_text\"]\n",
    "    df_cleaned = df[core_columns]\n",
    "    df_cleaned.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    # ---------------------- 6. ËæìÂá∫Ê∏ÖÊ¥óÊä•Âëä ----------------------\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\")\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°\")\n",
    "    print(f\"ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö{deep_clean_count}Êù°\")\n",
    "    print(f\"Êï∞ÊçÆ‰øùÁïôÁéáÔºö{deep_clean_count/original_count*100:.1f}%\")\n",
    "    print(f\"Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºö{output_path}\")\n",
    "    \n",
    "    # Â±ïÁ§∫Ââç2Êù°Ê∏ÖÊ¥óÊïàÊûúÂØπÊØî\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\")\n",
    "    for i in range(min(2, len(df_cleaned))):\n",
    "        print(f\"\\n„ÄêÂéüÂßãËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        original = df_cleaned.iloc[i][\"content\"]\n",
    "        print(f\"  {original[:100]}{'...' if len(original) > 100 else ''}\")\n",
    "        print(f\"„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        cleaned = df_cleaned.iloc[i][\"cleaned_content\"]\n",
    "        print(f\"  {cleaned[:100]}{'...' if len(cleaned) > 100 else ''}\")\n",
    "        print(f\"„ÄêÂàÜËØçÁªìÊûú{i+1}„ÄëÔºö\")\n",
    "        segmented = df_cleaned.iloc[i][\"segmented_text\"]\n",
    "        print(f\"  {segmented[:50]}{'...' if len(segmented) > 50 else ''}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# ---------------------- Áõ¥Êé•ËøêË°åÊ∏ÖÊ¥ó ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = clean_bilibili_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4291f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\82588\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂéüÂßãÊï∞ÊçÆÔºö1087Êù°ËØÑËÆ∫\n",
      "Âü∫Á°ÄËøáÊª§ÂêéÔºö1075Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\n",
      "Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö1059Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.838 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\n",
      "ÂéüÂßãÊï∞ÊçÆÔºö1087Êù°\n",
      "ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö1059Êù°\n",
      "Êï∞ÊçÆ‰øùÁïôÁéáÔºö97.4%\n",
      "Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºöD:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\n",
      "\n",
      "=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫1„ÄëÔºö\n",
      "  Á¨¨‰∏Ä[ÊòüÊòüÁúº]\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫1„ÄëÔºö\n",
      "  Á¨¨‰∏Ä\n",
      "„ÄêÂàÜËØçÁªìÊûú1„ÄëÔºö\n",
      "  Á¨¨‰∏Ä\n",
      "\n",
      "„ÄêÂéüÂßãËØÑËÆ∫2„ÄëÔºö\n",
      "  ÂâçÂçÅ\n",
      "„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫2„ÄëÔºö\n",
      "  ÂâçÂçÅ\n",
      "„ÄêÂàÜËØçÁªìÊûú2„ÄëÔºö\n",
      "  ÂâçÂçÅ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_bilibili_data(input_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/comment6.xlsx\", output_path=\"D:/CITYU COURSES/5507/homework/ËØÑËÆ∫/cleaned_bilibili.xlsx\"):\n",
    "    \"\"\"\n",
    "    ÂÆöÂà∂ÂåñÊ∏ÖÊ¥óBÁ´ôËØÑËÆ∫Êï∞ÊçÆÔºàÈÄÇÈÖç‰∏ä‰º†ÁöÑbilibili.xlsxÊñá‰ª∂Ôºâ\n",
    "    Ê†∏ÂøÉÂäüËÉΩÔºöÂéªÈáç„ÄÅÂéªÂô™„ÄÅÊñáÊú¨Ê†áÂáÜÂåñ„ÄÅÂàÜËØçÂ§ÑÁêÜ\n",
    "    \"\"\"\n",
    "    # ---------------------- 1. ËØªÂèñÂéüÂßãÊï∞ÊçÆ ----------------------\n",
    "    print(\"Ê≠£Âú®ËØªÂèñÊï∞ÊçÆ...\")\n",
    "    df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    original_count = len(df)\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°ËØÑËÆ∫\")\n",
    "\n",
    "    # ---------------------- 2. Âü∫Á°ÄÊï∞ÊçÆËøáÊª§ ----------------------\n",
    "    # 2.1 Âà†Èô§ÂÆåÂÖ®ÈáçÂ§çÁöÑËØÑËÆ∫\n",
    "    df = df.drop_duplicates(subset=[\"content\"])\n",
    "    # 2.2 Âà†Èô§ËØÑËÆ∫ÂÜÖÂÆπ‰∏∫Á©∫/NaNÁöÑÊù°ÁõÆ\n",
    "    df = df.dropna(subset=[\"content\"])\n",
    "    # 2.3 ËøáÊª§Â≠óÊï∞ËøáÁü≠ÁöÑÊó†ÊÑè‰πâËØÑËÆ∫ÔºàÂ∞ë‰∫é2Â≠óÔºâ\n",
    "    df = df[df[\"content\"].str.len() >= 2]\n",
    "    # 2.4 ÈáçÁΩÆÁ¥¢ÂºïÔºàÈÅøÂÖçÊ∏ÖÊ¥óÂêéÁ¥¢Âºï‰∏çËøûÁª≠Ôºâ\n",
    "    df = df.reset_index(drop=True)\n",
    "    basic_clean_count = len(df)\n",
    "    print(f\"Âü∫Á°ÄËøáÊª§ÂêéÔºö{basic_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§ÈáçÂ§ç/Á©∫ÂÄº/Áü≠ËØÑËÆ∫Ôºâ\")\n",
    "\n",
    "    # ---------------------- 3. Ê∑±Â∫¶ÊñáÊú¨Ê∏ÖÊ¥óÔºàÂéªÂô™+Ê†áÂáÜÂåñÔºâ ----------------------\n",
    "    def clean_single_comment(text):\n",
    "        \"\"\"ÂçïÊù°ËØÑËÆ∫Ê∏ÖÊ¥óÔºöÂéªÈô§BÁ´ôÁâπÊúâÂô™Â£∞ÔºåÁªü‰∏ÄÊ†ºÂºè\"\"\"\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # 3.1 ÂéªÈô§BÁ´ôÁâπÊÆäÂÖÉÁ¥†\n",
    "        text = re.sub(r\"\\[tv_\\w+\\]|\\[ÁÉ≠ËØçÁ≥ªÂàó_\\w+\\]|\\[\\w+\\]\", \"\", text)  # ÂéªÈô§Ë°®ÊÉÖÔºàÂ¶Ç[tv_ÁÇπËµû]Ôºâ\n",
    "        text = re.sub(r\"https?://\\S+|b23.tv/\\S+\", \"\", text)              # ÂéªÈô§URLÈìæÊé•ÔºàÂê´BÁ´ôÁü≠ÈìæÔºâ\n",
    "        text = re.sub(r\"@\\w+|#\\w+#\", \"\", text)                            # ÂéªÈô§@Áî®Êà∑Âíå#ËØùÈ¢òÊ†áÁ≠æ\n",
    "        \n",
    "        # 3.2 ÊñáÊú¨Ê†ºÂºèÊ†áÂáÜÂåñ\n",
    "        text = re.sub(r\"\\n+\", \"„ÄÇ\", text)  # Êç¢Ë°åÁ¨¶ËΩ¨‰∏∫Âè•Âè∑ÔºàÈÅøÂÖçÊñ≠Âè•Ê∑∑‰π±Ôºâ\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # Â§ö‰∏™Á©∫Ê†ºÂêàÂπ∂‰∏∫‰∏Ä‰∏™\n",
    "        text = text.replace(\",\", \"Ôºå\").replace(\".\", \"„ÄÇ\").replace(\"?\", \"Ôºü\").replace(\"!\", \"ÔºÅ\")  # Áªü‰∏Ä‰∏≠ÊñáÊ†áÁÇπ\n",
    "        \n",
    "        # 3.3 ‰øùÁïôÊúâÊïàÂ≠óÁ¨¶Ôºà‰ªÖ‰øùÁïô‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊï∞Â≠ó„ÄÅÂ∏∏Áî®‰∏≠ÊñáÊ†áÁÇπÔºâ\n",
    "        text = re.sub(r\"[^\\u4e00-\\u9fa5a-zA-Z0-9\\sÔºå„ÄÇÔºÅÔºüÔºõÔºö‚Äú‚Äù‚Äò‚ÄôÔºàÔºâ„Äê„Äë„ÄÅ]\", \"\", text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Â∫îÁî®Ê∏ÖÊ¥óÂáΩÊï∞ÔºåÊñ∞Â¢û‚ÄúÊ∏ÖÊ¥óÂêéËØÑËÆ∫‚ÄùÂ≠óÊÆµ\n",
    "    df[\"cleaned_content\"] = df[\"content\"].apply(clean_single_comment)\n",
    "    \n",
    "    # ËøáÊª§Ê∏ÖÊ¥óÂêé‰∏∫Á©∫/‰ªçËøáÁü≠ÁöÑËØÑËÆ∫ÔºàÂ¶ÇÂéüËØÑËÆ∫ÂÖ®ÊòØË°®ÊÉÖ/ÈìæÊé•Ôºâ\n",
    "    df = df[df[\"cleaned_content\"].str.len() >= 2]\n",
    "    df = df.reset_index(drop=True)\n",
    "    deep_clean_count = len(df)\n",
    "    print(f\"Ê∑±Â∫¶Ê∏ÖÊ¥óÂêéÔºö{deep_clean_count}Êù°ËØÑËÆ∫ÔºàÂéªÈô§Âô™Â£∞/Ê†áÂáÜÂåñÊ†ºÂºèÔºâ\")\n",
    "\n",
    "    # ---------------------- 4. ÂàÜËØçÂ§ÑÁêÜÔºà‰æø‰∫éÂêéÁª≠ÂàÜÊûêÔºâ ----------------------\n",
    "    # Âä†ËΩΩÂÅúÁî®ËØçË°®ÔºàËøáÊª§Êó†ÊÑè‰πâËØçÊ±áÔºå‰øùÁïôÊ†∏ÂøÉËØ≠‰πâÔºâ\n",
    "    stopwords = {\n",
    "        \"ÁöÑ\", \"‰∫Ü\", \"ÊòØ\", \"Êàë\", \"‰Ω†\", \"‰ªñ\", \"Â•π\", \"ÂÆÉ\", \"‰ª¨\", \"Âú®\", \"Âíå\", \"Â∞±\", \"ÈÉΩ\", \"ËÄå\", \"Âèä\", \"‰∏é\",\n",
    "        \"‰πü\", \"Ëøò\", \"‰∏ç\", \"Ê≤°\", \"Êúâ\", \"ÁùÄ\", \"Ëøá\", \"Ë¶Å\", \"‰ºö\", \"ËÉΩ\", \"ÂèØ\", \"Ëøô\", \"ÈÇ£\", \"Ê≠§\", \"ÂΩº\",\n",
    "        \"Âæà\", \"ÈùûÂ∏∏\", \"ÊØîËæÉ\", \"Â§™\", \"ÊúÄ\", \"Êõ¥\", \"Âèà\", \"ÂÜç\", \"Êâç\", \"Âè™\", \"‰ΩÜ\", \"Âç¥\", \"ËôΩ\", \"ÁÑ∂\",\n",
    "        \"Ôºå\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"Ôºö\", \"‚Äú\", \"‚Äù\", \"‚Äò\", \"‚Äô\", \"Ôºà\", \"Ôºâ\", \"„Äê\", \"„Äë\", \"„ÄÅ\", \" \",\n",
    "        \"ÂõûÂ§ç\", \"ÂºïÁî®\", \"‰∏æÊä•\", \"Âà†Èô§\", \"ÁºñËæë\", \"‰∏Ä‰∏™\", \"‰∏Ä‰∫õ\", \"‰∏ÄÁÇπ\", \"‰∏ÄÊ†∑\"\n",
    "    }\n",
    "\n",
    "    def segment_text(text):\n",
    "        \"\"\"‰∏≠ÊñáÂàÜËØçÂπ∂ËøáÊª§ÂÅúÁî®ËØç\"\"\"\n",
    "        words = jieba.lcut(text, cut_all=False)  # Á≤æÁ°ÆÂàÜËØç\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word not in stopwords \n",
    "            and len(word) >= 1 \n",
    "            and not (word.isdigit() and len(word) < 5)  # ‰øùÁïôÈïøÂ∫¶‚â•5ÁöÑÊï∞Â≠óÔºàÂ¶ÇÂπ¥‰ªΩÔºâ\n",
    "        ]\n",
    "        return filtered_words\n",
    "\n",
    "    # Êñ∞Â¢ûÂàÜËØçÁõ∏ÂÖ≥Â≠óÊÆµ\n",
    "    df[\"segmented_words\"] = df[\"cleaned_content\"].apply(segment_text)  # ÂàóË°®Ê†ºÂºèÔºà‰æø‰∫éÁ®ãÂ∫èÂ§ÑÁêÜÔºâ\n",
    "    df[\"segmented_text\"] = df[\"segmented_words\"].apply(lambda x: \" \".join(x))  # Â≠óÁ¨¶‰∏≤Ê†ºÂºèÔºà‰æø‰∫éÊü•ÁúãÔºâ\n",
    "\n",
    "    # ---------------------- 5. ‰øùÂ≠òÊ∏ÖÊ¥óÁªìÊûú ----------------------\n",
    "    # ‰øùÁïôÊ†∏ÂøÉÂ≠óÊÆµÔºöÂéüÂßãËØÑËÆ∫„ÄÅÊ∏ÖÊ¥óÂêéËØÑËÆ∫„ÄÅÂàÜËØçÁªìÊûúÔºàÂàóË°®Ôºâ„ÄÅÂàÜËØçÁªìÊûúÔºàÂ≠óÁ¨¶‰∏≤Ôºâ\n",
    "    core_columns = [\"content\", \"cleaned_content\", \"segmented_words\", \"segmented_text\"]\n",
    "    df_cleaned = df[core_columns]\n",
    "    df_cleaned.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    # ---------------------- 6. ËæìÂá∫Ê∏ÖÊ¥óÊä•Âëä ----------------------\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÂÆåÊàêÊä•Âëä ===\")\n",
    "    print(f\"ÂéüÂßãÊï∞ÊçÆÔºö{original_count}Êù°\")\n",
    "    print(f\"ÊúÄÁªàÊúâÊïàÊï∞ÊçÆÔºö{deep_clean_count}Êù°\")\n",
    "    print(f\"Êï∞ÊçÆ‰øùÁïôÁéáÔºö{deep_clean_count/original_count*100:.1f}%\")\n",
    "    print(f\"Ê∏ÖÊ¥óÂêéÊñá‰ª∂Ë∑ØÂæÑÔºö{output_path}\")\n",
    "    \n",
    "    # Â±ïÁ§∫Ââç2Êù°Ê∏ÖÊ¥óÊïàÊûúÂØπÊØî\n",
    "    print(f\"\\n=== Ê∏ÖÊ¥óÊïàÊûúÁ§∫‰æã ===\")\n",
    "    for i in range(min(2, len(df_cleaned))):\n",
    "        print(f\"\\n„ÄêÂéüÂßãËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        original = df_cleaned.iloc[i][\"content\"]\n",
    "        print(f\"  {original[:100]}{'...' if len(original) > 100 else ''}\")\n",
    "        print(f\"„ÄêÊ∏ÖÊ¥óÂêéËØÑËÆ∫{i+1}„ÄëÔºö\")\n",
    "        cleaned = df_cleaned.iloc[i][\"cleaned_content\"]\n",
    "        print(f\"  {cleaned[:100]}{'...' if len(cleaned) > 100 else ''}\")\n",
    "        print(f\"„ÄêÂàÜËØçÁªìÊûú{i+1}„ÄëÔºö\")\n",
    "        segmented = df_cleaned.iloc[i][\"segmented_text\"]\n",
    "        print(f\"  {segmented[:50]}{'...' if len(segmented) > 50 else ''}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# ---------------------- Áõ¥Êé•ËøêË°åÊ∏ÖÊ¥ó ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = clean_bilibili_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
